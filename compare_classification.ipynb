{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDbYV9wXtDG4",
        "outputId": "c839ba0f-da1a-4a2f-bf68-5dab7780afff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIdUW2p7tI4q",
        "outputId": "1ad6437a-d397-4f49-898c-271a764513ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Ihcr5rg-y_S_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "import optuna\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "rf_grid = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=3)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "# AdaBoost\n",
        "ada = AdaBoostClassifier()\n",
        "param_grid_ada = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "}\n",
        "ada_random = RandomizedSearchCV(estimator=ada, param_distributions=param_grid_ada, n_iter=10, cv=3)\n",
        "ada_random.fit(X_train, y_train)\n",
        "best_ada = ada_random.best_estimator_\n",
        "y_pred_ada = best_ada.predict(X_test)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier()\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "gb_random = RandomizedSearchCV(estimator=gb, param_distributions=param_grid_gb, n_iter=10, cv=3)\n",
        "gb_random.fit(X_train, y_train)\n",
        "best_gb = gb_random.best_estimator_\n",
        "y_pred_gb = best_gb.predict(X_test)\n",
        "\n",
        "# Support Vector Machine\n",
        "svm = SVC()\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "}\n",
        "svm_grid = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=3)\n",
        "svm_grid.fit(X_train, y_train)\n",
        "best_svm = svm_grid.best_estimator_\n",
        "y_pred_svm = best_svm.predict(X_test)\n",
        "\n",
        "# k-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "}\n",
        "knn_random = RandomizedSearchCV(estimator=knn, param_distributions=param_grid_knn, n_iter=10, cv=3)\n",
        "knn_random.fit(X_train, y_train)\n",
        "best_knn = knn_random.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier()\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "}\n",
        "xgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid_xgb, n_iter=10, cv=3)\n",
        "xgb_random.fit(X_train, y_train)\n",
        "best_xgb = xgb_random.best_estimator_\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "# LightGBM\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "param_grid_lgbm = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "}\n",
        "lgbm_random = RandomizedSearchCV(estimator=lgbm, param_distributions=param_grid_lgbm, n_iter=10, cv=3)\n",
        "lgbm_random.fit(X_train, y_train)\n",
        "best_lgbm = lgbm_random.best_estimator_\n",
        "y_pred_lgbm = best_lgbm.predict(X_test)\n",
        "\n",
        "# Neural Network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid_nn = {\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20],\n",
        "}\n",
        "nn_random = RandomizedSearchCV(estimator=nn, param_distributions=param_grid_nn, n_iter=4, cv=3)\n",
        "nn_random.fit(X_train, y_train)\n",
        "best_nn = nn_random.best_estimator_\n",
        "y_pred_nn = best_nn.predict(X_test)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"Support Vector Machine Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"k-Nearest Neighbors Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"LightGBM Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfMblsazuDZe",
        "outputId": "d9dc73dc-a964-4b56-c149-4a16caac9957"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "<ipython-input-9-e51e110430d9>:130: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  nn = KerasClassifier(build_fn=create_model, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "Logistic Regression Accuracy: 1.0\n",
            "Random Forest Accuracy: 1.0\n",
            "AdaBoost Accuracy: 1.0\n",
            "Gradient Boosting Accuracy: 1.0\n",
            "Support Vector Machine Accuracy: 1.0\n",
            "k-Nearest Neighbors Accuracy: 1.0\n",
            "Naive Bayes Accuracy: 1.0\n",
            "XGBoost Accuracy: 1.0\n",
            "LightGBM Accuracy: 1.0\n",
            "Neural Network Accuracy: 0.5333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
        "# Evaluation metrics\n",
        "print(\"Logistic Regression:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_logreg, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_logreg, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_logreg, average='weighted'))\n",
        "\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_rf, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_rf, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_rf, average='weighted'))\n",
        "\n",
        "print(\"\\nAdaBoost:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_ada, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_ada, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_ada, average='weighted'))\n",
        "\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_gb, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_gb, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_gb, average='weighted'))\n",
        "\n",
        "print(\"\\nSupport Vector Machine:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_svm, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_svm, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_svm, average='weighted'))\n",
        "\n",
        "print(\"\\nk-Nearest Neighbors:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_knn, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_knn, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_knn, average='weighted'))\n",
        "\n",
        "print(\"\\nNaive Bayes:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_nb, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_nb, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_nb, average='weighted'))\n",
        "\n",
        "print(\"\\nXGBoost:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_xgb, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_xgb, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_xgb, average='weighted'))\n",
        "\n",
        "print(\"\\nLightGBM:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_lgbm, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_lgbm, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_lgbm, average='weighted'))\n",
        "\n",
        "print(\"\\nNeural Network:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_nn, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_nn, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_nn, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPtkICgoy2XA",
        "outputId": "12831915-64a4-4acf-9aac-0e41fcc73dd5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Random Forest:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "AdaBoost:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Gradient Boosting:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Support Vector Machine:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "k-Nearest Neighbors:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Naive Bayes:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "XGBoost:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "LightGBM:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Neural Network:\n",
            "Accuracy: 0.5333333333333333\n",
            "Precision: 0.3688888888888889\n",
            "Recall: 0.5333333333333333\n",
            "F1-Score: 0.4352564102564102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}