{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ARRAYS"
      ],
      "metadata": {
        "id": "qfuH7XRLZibr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTu1bnqrZLlH"
      },
      "outputs": [],
      "source": [
        "# Import du module numpy sous l'alias 'np'\n",
        "import numpy as np\n",
        "\n",
        "# Création d'une matrice de dimensions 5x10 remplie de zéros\n",
        "X = np.zeros(shape = (5, 10))\n",
        "\n",
        "# Création d'une matrice à 3 dimensions 3x10x10 remplie de uns\n",
        "X = np.ones(shape = (3, 10, 10))\n",
        "\n",
        "# Création d'un array à partir d'une liste définie en compréhension\n",
        "X = np.array([2*i for i in range(10)])    # 0, 2, 4, 6, ..., 18\n",
        "\n",
        "# Création d'un array à partir d'une liste de listes\n",
        "X = np.array([[1, 3, 3],\n",
        "              [3, 3, 1],\n",
        "              [1, 2, 0]])\n",
        "\n",
        "# Création d'une matrice de dimensions 10x10 remplie de uns\n",
        "X = np.ones(shape = (10, 10))\n",
        "\n",
        "# affichage de l'élément à l'index (4, 3)\n",
        "print(X[4, 3])\n",
        "\n",
        "# assignation de la valeur -1 à l'élément d'index (1, 5)\n",
        "X[1, 5] = -1\n",
        "\n",
        "# Insérez votre code ici\n",
        "\n",
        "X = np.zeros(shape = (6, 6))\n",
        "\n",
        "for i in range(6):\n",
        "    X[i,:] = np.array([0, 1, 2, 3, 4, 5])\n",
        "\n",
        "X = np.array([i/100 for i in range(100)])  # 0, 0.01, 0.02, 0.03, ..., 0.98, 0.99\n",
        "\n",
        "# Calcul de l'exponentielle de x pour x = 0, 0.01, 0.02, 0.03, ..., 0.98, 0.99\n",
        "exp_X = np.exp(X)\n",
        "\n",
        "\n",
        "'''\n",
        "Comme vous pouvez le voir, le temps de calcul avec une boucle for est extrêmement long.\n",
        "\n",
        "C'est pourquoi il est toujours préférable d'effectuer des calculs sur des matrices à l'aide de numpy plutôt qu'avec des boucles. Ce sera le cas lorsque nous feront des statistiques sur des données.\n",
        "''''\n",
        "#avec for\n",
        "def f_python(X):\n",
        "    n = X.shape[0]\n",
        "    for i in range(n):\n",
        "        X[i] = np.exp(np.sin(X[i]) + np.cos(X[i]))\n",
        "\n",
        "#avec numpy\n",
        "X = np.array([i/100 for i in range(100)])\n",
        "\n",
        "def f(X):\n",
        "    return np.exp(np.sin(X) + np.cos(X))\n",
        "\n",
        "print(f(X))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[-1, 0, 30],\n",
        "              [-2, 3, -5],\n",
        "              [5, -5, 10]])\n",
        "\n",
        "# On assigne à tous les éléments négatifs la valeur 0\n",
        "X[X<0] = 0\n",
        "\n",
        "# Affichage de la matrice modifiée\n",
        "print(X)\n",
        "\n",
        "Ceci permet d'accéder et modifier facilement des éléments qui respectent une condition spécifique.\n",
        "\n",
        "De plus, il est possible d'indexer un array à l'aide d'une condition évaluée sur un autre array :\n",
        "\n",
        "\n",
        "# Création de 2 arrays à 8 éléments\n",
        "X = np.array([3, -7, -10, 3, 6, 5, 2, 9])\n",
        "\n",
        "y = np.array([0, 1, 1, 1, 0, 1, 0, 0])\n",
        "\n",
        "# On assigne la valeur -1 aux éléments de X pour lesquels la valeur de y à l'indice correspondant vaut 1\n",
        "X[y == 1] = -1\n",
        "\n",
        "# Affichage de X\n",
        "print(X)\n",
        ">>> [3 -1 -1 -1 6 -1 2 9]\n",
        "\n",
        "# Affichage des éléments de X pour lesquels la valeur de y à l'indice correspondant vaut 0\n",
        "print(X[y == 0])\n",
        ">>> [3 6 2 9]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "_B5u14P8eabq",
        "outputId": "4687e287-7f93-4bee-c55a-2a47ea92ad57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-afcfe018d779>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    Ceci permet d'accéder et modifier facilement des éléments qui respectent une condition spécifique.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Appliquer une des opérations arithmétiques de base (/, *, -, +, **) entre un tableau et une valeur, appliquera l'opération à chacun des éléments du tableau.\n",
        "Il est également possible de faire une opération arithmétique entre deux tableaux. Cela appliquera l'opération entre chaque paire d'éléments.\n",
        "# Création de deux arrays à 2 valeurs\n",
        "a = np.array([4, 10])\n",
        "b = np.array([6, 7])\n",
        "\n",
        "# Multiplication entre deux arrays\n",
        "print(a * b)\n",
        ">>> [24 70]\n",
        "\n",
        "\n",
        ">>> for count, value in enumerate(values):\n",
        "...     print(count, value)\n",
        "\n",
        "\n",
        "(a) Importer le package numpy sous le nom np.\n",
        "(b) Créer un array de dimensions 10x4 rempli de 1.\n",
        "(c) À l'aide d'une boucle for et de la fonction enumerate, multiplier chaque ligne par son indice. Afin de modifier la matrice, il faut que vous y accédiez par indexation.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "M = np.ones((10,4))\n",
        "\n",
        "# Pour chaque ligne de la matrice M\n",
        "for i, row in enumerate(M):\n",
        "    # On multiplie la ligne par son indice\n",
        "    M[i, :] = row * i\n",
        "    # Alternativement M[i, :] *= i\n",
        "\n",
        "print(M)\n",
        "\n",
        "\n",
        "M = np.array([[5, 1],\n",
        "              [3, 0]])\n",
        "\n",
        "N = np.array([[2, 4],\n",
        "              [0, 8]])\n",
        "\n",
        "# Produit matriciel entre les deux arrays\n",
        "print(M.dot(N))\n",
        ">>> [[10 28]\n",
        ">>>  [ 6 12]]"
      ],
      "metadata": {
        "id": "8ZjdTE4-hp1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def puissanceA(n):\n",
        "    # A est initialisé à la matrice identité\n",
        "    A = np.array([[1, 0],\n",
        "                  [0, 1]])\n",
        "\n",
        "    # Cette matrice B servira à calculer les puissances de A\n",
        "    B = np.array([[1, -1],\n",
        "                  [-1, 1]])\n",
        "\n",
        "    # On mutlpilie A par B n fois pour obtenir A**n\n",
        "    for i in range(n):\n",
        "        A = A.dot(B)\n",
        "\n",
        "    return A\n",
        "\n",
        "print(\"A**2: \\n\", puissanceA(2), \"\\n\")\n",
        "print(\"A**3: \\n\", puissanceA(3), \"\\n\")\n",
        "print(\"A**4: \\n\", puissanceA(4), \"\\n\")\n",
        "\n",
        "print(\"Une formule générale de A**n est donné par:\")\n",
        "print(\"[2**(n-1), -2**(n-1)]\")\n",
        "print(\"[-2**(n-1), 2**(n-1)]\")"
      ],
      "metadata": {
        "id": "4RpNwNUskx8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insérez votre code ici\n",
        "\n",
        "#nornlaiser la matrice\n",
        "# La normalisation Min-Max est une méthode qui s'utilise pour rééchelonner les variables d'une base de données dans la plage  [0,1]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[24,1.88],\n",
        "            [18,1.68],\n",
        "            [14,1.65]])\n",
        "\n",
        "\n",
        "X2 = (X-np.min(X, axis = 0))/((np.max(X, axis = 0)-np.min(X, axis = 0)))\n",
        "\n",
        "print(X2)\n"
      ],
      "metadata": {
        "id": "R8rUNVx6rZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1, 1, 10],\n",
        "              [3, 5, 2]])\n",
        "\n",
        "# Calcul de la moyenne sur TOUTES les valeurs de X\n",
        "print(A.mean())\n",
        ">>> 3.67\n",
        "\n",
        "# Calcul de la moyenne sur les COLONNES de X\n",
        "print(A.mean(axis = 0))\n",
        ">>> [2. 3. 6.]\n",
        "\n",
        "# Calcul de la moyenne sur les LIGNES de X\n",
        "print(A.mean(axis = 1))\n",
        ">>> [4. 3.33]"
      ],
      "metadata": {
        "id": "gD9X2l_Frl0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(X, beta, y):\n",
        "    # Calcul de ^y\n",
        "    y_hat = X.dot(beta)\n",
        "\n",
        "    # Calcul de l'erreur quadratique (^y_i - y_i)**2\n",
        "    mse = (y_hat - y)**2\n",
        "\n",
        "    # On fait la moyenne de l'erreur pour obtenir la MSE\n",
        "    mse = mse.mean()\n",
        "\n",
        "    return mse"
      ],
      "metadata": {
        "id": "l_xcbwrmtJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([24,\n",
        "              18,\n",
        "              14])\n",
        "\n",
        "y = np.array([1.88,\n",
        "              1.68,\n",
        "              1.65])\n",
        "\n",
        "# Liste contenant les mse\n",
        "errors = []\n",
        "\n",
        "# Liste contenant les betas à tester\n",
        "betas = np.linspace(start = 0.01, stop = 0.15, num = 15)\n",
        "\n",
        "# Pour beta allant de 0.01 à\n",
        "for beta in betas:\n",
        "    errors.append(mean_squared_error(X, beta, y))\n"
      ],
      "metadata": {
        "id": "MNAdBI0jvuis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Array contenant les MSE pour chaque beta\n",
        "errors = np.array(errors)\n",
        "\n",
        "# Liste contenant les betas que nous avons testé\n",
        "betas = np.linspace(start = 0.01, stop = 0.15, num = 15)\n",
        "\n",
        "# Indice de la MSE minimale\n",
        "index_beta_optimal = errors.argmin()\n",
        "\n",
        "# On récupère le beta optimal grâce à l'indice\n",
        "beta_optimal = betas[index_beta_optimal]\n",
        "\n",
        "print(\"Le beta optimal est:\", beta_optimal)"
      ],
      "metadata": {
        "id": "3UUflUyKvynL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation du module pandas sous le nom pd\n",
        "import pandas as pd\n",
        "\n",
        "# Chargement de la base transactions\n",
        "transactions = pd.read_csv(\"transactions.csv\", sep =',', index_col = \"transaction_id\")\n",
        "\n",
        "# Affichage des 10 premières lignes de transactions\n",
        "transactions.head(10)"
      ],
      "metadata": {
        "id": "jBzBxCcDFJUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut directement spécifier le nom de la colonne contenant les indices\n",
        "\n",
        "transactions = pd.read_csv(filepath_or_buffer = 'transactions.csv',    # chemin du fichier\n",
        "                           sep = ',',                    # caractère séparant les valeurs\n",
        "                           header = 0,                   # numéro de la ligne contenant le nom des colonnes\n",
        "                           index_col = 'transaction_id') # nom de la colonne qui indexe les entrées\n",
        "\n",
        "\n",
        "# On peut aussi directement renseigner le numéro de la colonne qui indexe les entrées\n",
        "\n",
        "transactions = pd.read_csv(filepath_or_buffer = 'transactions.csv',\n",
        "                           sep = ',',\n",
        "                           header = 0,\n",
        "                           index_col = 0)  # numéro de la colonne qui indexe les entrées"
      ],
      "metadata": {
        "id": "6aLtmBd0zEkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.head(20)\n",
        "transactions.tail\n",
        "transactions.shape"
      ],
      "metadata": {
        "id": "Stod-R2Uzc8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_last = df.drop_duplicates(keep = 'last')"
      ],
      "metadata": {
        "id": "UqkL8aq_xKuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des données\n",
        "transactions = pd.read_csv(\"transactions.csv\", sep =',', index_col = \"transaction_id\")\n",
        "\n",
        "# Suppression des doublons\n",
        "transactions = transactions.drop_duplicates(keep = 'first')\n",
        "\n",
        "## Exercice\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Remplacement des modalités\n",
        "transactions = transactions.replace(to_replace = ['e-Shop', 'TeleShop', 'MBR', 'Flagship store',  np.nan],\n",
        "                                    value= [1, 2, 3, 4, 0])\n",
        "\n",
        "# Conversion des types des colonnes\n",
        "new_types = {'Store_type'       : 'int',\n",
        "             'prod_subcat_code' : 'int'}\n",
        "\n",
        "transactions = transactions.astype(new_types)\n",
        "\n",
        "# Changement de nom des colonnes\n",
        "new_names =  {'Store_type' : 'store_type',\n",
        "              'Qty'        : 'qty',\n",
        "              'Rate'       : 'rate',\n",
        "              'Tax'        : 'tax'}\n",
        "\n",
        "transactions = transactions.rename(new_names, axis = 1)\n",
        "\n",
        "# Affichage des premières lignes de transactions\n",
        "transactions.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YOjlWjt12I25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "La date '28-02-2014' est une chaîne de caractères. Le jour, le mois et l'année sont séparées par un tiret '-'.\n",
        "La classe des chaînes de caractères dispose de la méthode split pour découper une chaîne sur un caractère spécifique :\n",
        "\n",
        "date = '28-02-2014'\n",
        "\n",
        "# Découpage de la chaîne sur le caractère '-'\n",
        "print(date.split('-'))\n",
        ">>> ['28', '02', '2014']\n",
        "\n",
        "es fonctions à appliquer à la colonne 'tran_date'\n",
        "def get_day(date):\n",
        "    \"\"\"\n",
        "    Prend en argument une date sous forme de chaîne de caractères.\n",
        "\n",
        "    La date doit avoir le format 'JJ-MM-AAAA'.\n",
        "\n",
        "    Cette fonction renvoie le jour (JJ).\n",
        "        \"\"\"\n",
        "\n",
        "    # Découpage de la chaîne sur le caractère '-'\n",
        "    splits = date.split('-')\n",
        "\n",
        "    # On renvoie le premier élément du découpage (jour)\n",
        "    day = splits[0]\n",
        "    return day\n",
        "\n",
        "def get_month(date):\n",
        "    return date.split('-')[1]\n",
        "\n",
        "def get_year(date):\n",
        "    return date.split('-')[2]\n",
        "\n",
        "\n",
        "# Application des fonctions\n",
        "days = transactions['tran_date'].apply(get_day)\n",
        "months = transactions['tran_date'].apply(get_month)\n",
        "years = transactions['tran_date'].apply(get_year)\n",
        "\n",
        "# Création des nouvelles colonnes\n",
        "transactions['day'] = days\n",
        "transactions['month'] = months\n",
        "transactions['year'] = years\n",
        "\n",
        "# Affichage des premières lignes de transactions\n",
        "transactions.head()\n",
        "\n",
        "\n",
        "#: or\n",
        "transactions['day'] = transactions['tran_date'].apply(lambda date: date.split('-')[0])\n",
        "\n",
        "\n",
        "# Using Dataframe.apply() to apply function to every row\n",
        "def add(row):\n",
        "   return row[0]+row[1]+row[2]\n",
        "\n",
        "df['new_col'] = df.apply(add, axis=1)\n",
        "\n",
        "# Using lambda function\n",
        "df['new_col'] = df.apply(lambda row : row[0]+row[1]+row[2], axis=1)\n"
      ],
      "metadata": {
        "id": "F1iVzUse6BCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On détecte les COLONNES contenant au moins une valeur manquante\n",
        "df.isna().any(axis = 0)\n",
        ">>> Nom      True\n",
        ">>> Pays     False\n",
        ">>> Age      True\n",
        "# On détecte les LIGNES contenant au moins une valeur manquante\n",
        "df.isna().any(axis = 1)\n",
        ">>> 0     True\n",
        ">>> 1    False\n",
        ">>> 2    False\n",
        "# On utilise l'indexation conditionnelle pour afficher les entrées\n",
        "# contenant des valeurs manquantes\n",
        "\n",
        "df[df.isna().any(axis = 1)]\n",
        "ce qui renvoie le DataFrame :\n",
        "\n",
        "\n",
        "\n",
        "Nom\tPays\tAge\n",
        "0\tNaN\tAustralie\tNaN\n",
        "# On compte le nombre de valeurs manquantes pour chaque COLONNE\n",
        "df.isnull().sum(axis = 0) #Les fonctions isnull et isna sont strictement équivalentes\n",
        ">>> Nom     1\n",
        ">>> Pays    0\n",
        ">>> Age     1\n",
        "# On compte le nombre de valeurs manquantes pour chaque LIGNE\n",
        "df.isnull().sum(axis = 1)\n",
        ">>> 0    2\n",
        ">>> 1    0\n",
        ">>> 2    0"
      ],
      "metadata": {
        "id": "zZ-TB5MchDd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emplacement des valeurs manquantes (méthode fillna)\n",
        "La méthode fillna permet de remplacer les valeurs manquantes d'un DataFrame par des valeurs de notre choix.\n",
        "\n",
        "# On remplace tous les NANs du DataFrame par des zéros\n",
        " df.fillna(0)\n",
        "\n",
        "# On remplace les NANs de chaque colonne numérique par la moyenne sur cette colonne\n",
        " df.fillna(df.mean())  # df.mean() peut être remplacée par n'importe quelle méthode statistique.\n",
        "Il est courant de remplacer les valeurs manquantes d'une colonne de type numérique avec des statistiques comme :\n",
        "\n",
        "La moyenne : mean.\n",
        "La médiane : median.\n",
        "Le minimum/maximum : min/max.\n",
        "Pour les colonnes de type catégorielle, on remplacera les valeurs manquantes avec :\n",
        "\n",
        "Le mode, i.e. la modalité la plus fréquente : mode.\n",
        "Une constante ou catégorie arbitraire : 0, -1.\n",
        "Pour éviter de faire des erreurs de remplacement, il est fortement conseillé de sélectionner les bonnes colonnes avant d'utiliser la méthode fillna.\n",
        "\n",
        "Si vous faites des erreurs dans l'exercice suivant, vous pouvez réimporter transactions à l'aide de la cellule suivante :"
      ],
      "metadata": {
        "id": "x5F-uIeyhY94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L'en-tête de la méthode est la suivante : dropna(axis, how, subset, ..)\n",
        "\n",
        "Le paramètre axis précise si on doit supprimer des lignes ou des colonnes (0 pour les lignes, 1 pour les colonnes).\n",
        "Le paramètre how permet de préciser comment les lignes (ou les colonnes) sont supprimées :\n",
        "how = 'any': On supprime la ligne (ou colonne) si elle contient au moins une valeur manquante.\n",
        "how = 'all' : On supprime la ligne (ou colonne) si elle ne contient que des valeurs manquantes.\n",
        "Le paramètre subset permet de préciser les colonnes/lignes sur lesquelles on effectue la recherche de valeurs manquantes.\n",
        "Exemple :\n",
        "# On supprime toutes les lignes contenant au moins une valeur manquante\n",
        "df = df.dropna(axis = 0, how = 'any')\n",
        "\n",
        "# On supprime les colonnes vides\n",
        "df = df.dropna(axis = 1, how = 'all')\n",
        "\n",
        "# On supprime les lignes ayant des valeurs manquantes dans les 3 colonnes 'col2','col3' et 'col4'\n",
        " df.dropna(axis = 0, how = 'all', subset = ['col2','col3','col4'])"
      ],
      "metadata": {
        "id": "K2nQIfP3hkAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer = pd.read_csv('customer.csv')\n",
        "prod_cat_info = pd.read_csv('prod_cat_info.csv')\n",
        "\n",
        "#fill with mode (most often value seen)\n",
        "customer['Gender'] = customer['Gender'].fillna(customer['Gender'].mode()[0])\n",
        "customer['city_code'] = customer['city_code'].fillna(customer['city_code'].mode()[0])"
      ],
      "metadata": {
        "id": "h6Nm1x3PUoq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion et récap\n",
        "Dans ce chapitre nous avons vu les méthodes essentielles du module pandas afin de nettoyer un dataset et gérer les valeurs manquantes (NaN).\n",
        "\n",
        "Cette étape de préparation d'un dataset est toujours la première étape d'un projet data.\n",
        "\n",
        "Concernant le nettoyage des données, nous avons ainsi appris à :\n",
        "\n",
        "Repérer et supprimer les doublons d'un DataFrame grâce aux méthodes duplicated et drop_duplicates.\n",
        "Modifier les éléments d'un DataFrame et leur type à l'aide des méthodes replace, rename et astype.\n",
        "Appliquer une fonction à un DataFrame avec la méthode apply et la clause lambda.\n",
        "Concernant la gestion des valeurs manquantes, nous avons appris à :\n",
        "\n",
        "Les détecter grâce à la méthode isna suivie des méthodes any et sum.\n",
        "Les remplacer à l'aide de la méthode fillna et des méthodes statistiques.\n",
        "Les supprimer grâce à la méthode dropna.\n",
        "Dans le notebook suivant, vous verrez d'autres manipulations de DataFrame pour une exploration des données plus avancées."
      ],
      "metadata": {
        "id": "IW65oO3nhoPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séparation du DataFrame transactions\n",
        "part_1 = transactions[transactions.columns[:4]]\n",
        "part_2 = transactions[transactions.columns[4:]]\n",
        "\n",
        "# Reconstitution du DataFrame transactions par concaténation\n",
        "union = pd.concat([part_1,part_2], axis = 1)"
      ],
      "metadata": {
        "id": "XKM1FCTZXdXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filtrage du DataFrame sur les 2 conditions précédentes\n",
        "print(df[(df['annee'] == 1979) & (df['surface'] > 60)])\n",
        ">>>           quartier  annee  surface\n",
        ">>> 0  Champs-Elysées   1979       70\n",
        "Les conditions doivent être renseignées entre parenthèses pour éliminer l'ambigüité sur l'ordre d'évaluation des conditions.\n",
        "En effet, si les conditions ne sont pas proprement séparées, nous obtiendrons l'erreur suivante :\n",
        "\n",
        "print(df[df['annee'] == 1979 & df['surface'] > 60])\n",
        ">>> ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
        "\n",
        "# Filtrage du DataFrame sur les 2 conditions précédentes: Si nous souhaitons retrouver un appartement qui date d'après 1900 ou qui est situé dans le quartier de Père-Lachaise, nous pouvons filtrer les lignes de df avec le code suivant :\n",
        "print(df[(df['année'] > 1900) | (df['quartier'] == 'Père-Lachaise')])\n",
        "\n",
        "Si nous souhaitons un appartement ne se situant pas dans le quartier de Bercy alors on filtre df de la manière suivante :\n",
        "\n",
        "\n",
        "# Filtrage du DataFrame sur les 2 conditions précédentes\n",
        "print(df[-(df['quartier'] == 'Bercy')])\n"
      ],
      "metadata": {
        "id": "maNKn9QsP8Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "576vMmRI59Wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ramètre how peut prendre 4 valeurs ('inner', 'outer', 'left', 'right') que nous allons illustrer sur les deux DataFrames nommés Personnes et Vehicule suivants :\n",
        "\n",
        "Nom\tVoiture\n",
        "Lila\tTwingo\n",
        "Tiago\tClio\n",
        "Berenice\tC4 Cactus\n",
        "Joseph\tTwingo\n",
        "Kader\tSwift\n",
        "Romy\tScenic\n",
        "Voiture\tPrix\n",
        "Twingo\t11000\n",
        "Swift\t14500\n",
        "C4 Cactus\t23000\n",
        "Clio\t16000\n",
        "Prius\t30000\n",
        "- 'inner' : La jointure interne retourne les lignes dont les valeurs dans les colonnes communes sont présentes dans les deux DataFrames. Ce type de jointure est souvent déconseillé car il peut amener à la perte de beaucoup d'entrées. Par contre, la jointure interne ne produit aucun NaN.\n",
        "Le résultat de la jointure interne Personnes.merge(right = Vehicule, on = 'Voiture', how = 'inner') sera :\n",
        "\n",
        "\n",
        "\n",
        "- 'outer' : La jointure externe fusionne la totalité des deux DataFrames. Aucune ligne ne sera supprimée. Cette méthode peut générer énormément de NaNs.\n",
        "Le résultat de la jointure externe Personnes.merge(right = Vehicule, on = 'Voiture', how = 'outer') sera :\n",
        "\n",
        "\n",
        "\n",
        "- 'left' : La jointure à gauche retourne toutes les lignes du DataFrame de gauche, et les complète avec les lignes du second DataFrame qui coïncident selon les valeurs de la colonne commune. C'est la valeur par défaut du paramètre how.\n",
        "Le résultat de la jointure à gauche Personnes.merge(right = Vehicule, on = 'Voiture', how = 'left') sera :\n",
        "\n",
        "\n",
        "\n",
        "- 'right' : La jointure à droite retourne toutes les lignes du DataFrame de droite, et les complète avec les lignes du DataFrame de gauche qui coïncident selon les indices de la colonne commune.\n",
        "Le résultat de la jointure à droite Personnes.merge(right = Vehicule, on = 'Voiture', how = 'right') sera :\n",
        "\n",
        "\n",
        "\n",
        "Faire une jointure à gauche, une jointure à droite ou une jointure externe suivie d'un dropna(how = 'any') est équivalent à une jointure interne."
      ],
      "metadata": {
        "id": "3AxDBgtBYPvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On renomme la colonne 'customer_Id' en 'cust_id' pour faire la fusion\n",
        "customer = customer.rename(columns = {'customer_Id':'cust_id'})\n",
        "\n",
        "# Jointure à gauche entre transactions et customer sur la colonne 'cust_id'\n",
        "fusion = transactions.merge(right = customer, on = 'cust_id', how = 'left')\n",
        "\n",
        "# La fusion n'a produit aucun NaN\n",
        "fusion.isna().sum()\n",
        "\n",
        "# Les colonnes DOB, Gender, city_code ont bien été ajoutées à transactions\n",
        "fusion.head()"
      ],
      "metadata": {
        "id": "fhfhlhICrxXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On peut définir la colonne 'Nom' comme étant le nouvel index :\n",
        "\n",
        "df = df.set_index('Nom')\n",
        "\n",
        "# On récupère l'index de transactions\n",
        "new_index = transactions.index\n",
        "\n",
        "# On définit le nouvel index de fusion\n",
        "fusion = fusion.set_index(new_index)\n",
        "fusion.head()"
      ],
      "metadata": {
        "id": "dF-m9aFJtqfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "n_modalities = lambda store_type: len(np.unique(store_type))\n",
        "\n",
        "functions_to_apply = {\n",
        "    # Les méthodes statistiques classiques peuvent être renseignées avec\n",
        "    # chaines de caractères\n",
        "    'total_amt' : ['min', 'max', 'sum'],\n",
        "    'store_type' : n_modalities\n",
        "}\n",
        "\n",
        "transactions.groupby('cust_id').agg(functions_to_apply)\n",
        "\n",
        "# Quantité maximale\n",
        "max_qty = lambda qty: qty[qty > 0].max()\n",
        "\n",
        "# Quantité minimale\n",
        "min_qty = lambda qty: qty[qty > 0].min()\n",
        "\n",
        "# Quantité médiane\n",
        "median_qty = lambda qty : qty[qty > 0].median()\n",
        "\n",
        "# Définition du dictionnaire de fonctions à appliquer\n",
        "functions_to_apply = {\n",
        "    'qty' : [max_qty, min_qty, median_qty]\n",
        "}\n",
        "\n",
        "# Operation groupby\n",
        "qty_groupby = transactions.groupby('cust_id').agg(functions_to_apply)\n",
        "\n",
        "# Pour un meilleur affichage, on peut renommer les colonnes produite par le groupby\n",
        "qty_groupby.columns.set_levels(['max_qty', 'min_qty', 'median_qty'], level=1, inplace = True)\n",
        "\n",
        "# Affichage des premières lignes du Dataframe produit par l'opération groupby\n",
        "qty_groupby.head()"
      ],
      "metadata": {
        "id": "cstz9zsl3y_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Filtrer les lignes d'un DataFrame avec plusieurs conditions grâce aux opérateurs binaires &, | et - :\n",
        "# Année égale à 1979 et surface supérieure à 60\n",
        "df[(df['annee'] == 1979) & (df['surface'] > 60)]\n",
        "\n",
        "# Année supérieure à 1900 ou quartier égal à 'Père-Lachaise'\n",
        "df[(df['année'] > 1900) | (df['quartier'] == 'Père-Lachaise')]\n",
        "\n",
        "Fusionner des DataFrames grâce à la fonction concat et la méthode merge :\n",
        "# Concaténation verticale\n",
        "pd.concat([df1, df2], axis = 0)\n",
        "\n",
        "# Concaténation horizontal\n",
        "pd.concat([df1, df2], axis = 1)\n",
        "\n",
        "# Différents types de jointures\n",
        "df1.merge(right = df2, on = 'column', how = 'inner')\n",
        "df1.merge(right = df2, on = 'column', how = 'outer')\n",
        "df1.merge(right = df2, on = 'column', how = 'left')\n",
        "df1.merge(right = df2, on = 'column', how = 'right')\n",
        "\n",
        "Trier et ordonner les valeurs d'un DataFrame grâce aux méthodes sort_values et sort_index :\n",
        "# Tri d'un DataFrame par la colonne 'column' dans l'ordre croissant\n",
        "df.sort_values(by = 'column', ascending = True)\n",
        "\n",
        "Effectuer une opération groupby complexe grâce aux fonctions lambda et aux méthodes groupby et agg :\n",
        "functions_to_apply = {\n",
        "    'column1' : ['min', 'max'],\n",
        "    'column2' : [np.mean, np.std],\n",
        "    'column3' : lambda x: x.max() - x.min()\n",
        "    }\n",
        "\n",
        "df.groupby('column_to_group_by').agg(functions_to_apply)"
      ],
      "metadata": {
        "id": "XIDKJrwURw3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'column_to_group_by': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "    'numeric_column1': [10, 20, 30, 40, 50, 60],\n",
        "    'numeric_column2': [2, 4, 6, 8, 10, 12],\n",
        "    'numeric_column3': [100, 200, 300, 400, 500, 600],\n",
        "    'string_column': ['X', 'Y', 'Z', 'X', 'Y', 'Z']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Group by 'column_to_group_by' and calculate the mean of 'numeric_column'\n",
        "mean_grouped = df.groupby('column_to_group_by').agg({'numeric_column1': 'mean'})\n",
        "print(\"Mean Grouped:\")\n",
        "print(mean_grouped)\n",
        "\n",
        "# Group by 'column_to_group_by' and 'string_column' and calculate the sum of 'numeric_column'\n",
        "sum_grouped = df.groupby(['column_to_group_by', 'string_column']).agg({'numeric_column1': 'sum'})\n",
        "print(\"\\nSum Grouped:\")\n",
        "print(sum_grouped)\n",
        "\n",
        "# Group by 'column_to_group_by' and calculate the sum of 'numeric_column1',\n",
        "# the mean of 'numeric_column2', and the maximum value of 'numeric_column3'\n",
        "multiple_grouped = df.groupby('column_to_group_by').agg({\n",
        "    'numeric_column1': 'sum',\n",
        "    'numeric_column2': 'mean',\n",
        "    'numeric_column3': 'max'\n",
        "})\n",
        "print(\"\\nMultiple Grouped:\")\n",
        "print(multiple_grouped)\n",
        "\n",
        "# Group by 'column_to_group_by' and apply custom aggregation functions\n",
        "custom_grouped = df.groupby('column_to_group_by').agg({\n",
        "    'numeric_column1': [np.sum, np.mean],\n",
        "    'string_column': lambda x: ','.join(x)\n",
        "})\n",
        "print(\"\\nCustom Grouped:\")\n",
        "print(custom_grouped)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWHDNW8vSxP8",
        "outputId": "52fd9bfc-9f53-4b91-8848-110a6d80e482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Grouped:\n",
            "                    numeric_column1\n",
            "column_to_group_by                 \n",
            "A                              15.0\n",
            "B                              35.0\n",
            "C                              55.0\n",
            "\n",
            "Sum Grouped:\n",
            "                                  numeric_column1\n",
            "column_to_group_by string_column                 \n",
            "A                  X                           10\n",
            "                   Y                           20\n",
            "B                  X                           40\n",
            "                   Z                           30\n",
            "C                  Y                           50\n",
            "                   Z                           60\n",
            "\n",
            "Multiple Grouped:\n",
            "                    numeric_column1  numeric_column2  numeric_column3\n",
            "column_to_group_by                                                   \n",
            "A                                30              3.0              200\n",
            "B                                70              7.0              400\n",
            "C                               110             11.0              600\n",
            "\n",
            "Custom Grouped:\n",
            "                   numeric_column1       string_column\n",
            "                               sum  mean      <lambda>\n",
            "column_to_group_by                                    \n",
            "A                               30  15.0           X,Y\n",
            "B                               70  35.0           Z,X\n",
            "C                              110  55.0           Y,Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Produits.loc[Produits['Categorie'] == 'Categorie 2', 'Prix unitaire']\n",
        "\n",
        "\tIndex\tNom_Produit\tCategorie\tCout unitaire\tPrix unitaire\n",
        "0\t1\tProduit 1\tCategorie 2\t1441.73\t2304.64\n",
        "1\t2\tProduit 2\tCategorie 3\t1418.57\t2285.53\n",
        "2\t3\tProduit 3\tCategorie 1\t1416.31\t2194."
      ],
      "metadata": {
        "id": "vxDtkw7XQ4Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_content = pd.merge(df_netflix, df_imdb, left_on = ['title','release_year'] , right_on = ['primaryTitle','startYear'])\n"
      ],
      "metadata": {
        "id": "T_dsvb582BJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# si il y action dans la cell\n",
        "#(g) Add to all_content an \"is_action\" column which will take the value True if a content belongs to the Action category, and False otherwise.\n",
        "all_content['is_action'] = all_content['listed_in'].apply(lambda x : 'Action' in x)\n",
        "\n",
        "\n",
        "#(h) Extract from all_content a DataFrame named all_movies, containing only movies (contents of type \"Movie\").\n",
        "all_movies = all_content[ all_content['type'] == 'Movie']\n",
        "#or\n",
        "all_movies = all_movies.loc[all_movies['type'] == 'Movie', ,:]\n",
        "\n",
        "#  Create, from the \"duration\" variable, a new \"duration_int\" variable in all_movies, where the last four characters will be truncated. Convert the type of the \"duration_int\" variable to int.\n",
        "all_movies['duration_int'] = all_movies['duration'].apply(lambda x: x[:-4]).astype(int)\n",
        "\n",
        "# ATTENTION TO BOOLEANS, are not strings !!!!!\n",
        "us_movies = all_movies[(all_movies['country'] == 'United States') & (all_movies['duration_int']< 160) & (all_movies['is_action'])]\n",
        "\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "us_movies = all_movies[(all_movies['country'] == 'United States') & (all_movies['duration_int']< 160) & (all_movies['is_action'])]\n",
        "\n",
        "duration = us_movies['duration_int']\n",
        "avg_rating = us_movies['averageRating']\n",
        "sns.lineplot(duration,avg_rating)\n",
        "\n",
        "#or\n",
        "sns.lineplot(x='duration_int', y='averageRating', data = us_movies)"
      ],
      "metadata": {
        "id": "aB6iQIsO6wJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##CAREFUL\n",
        "\n",
        "S = pd.Series([\"hello friend\", \"hello world\", \"hi\"])\n",
        "S.str.split(' ', expand=True).stack().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AqLd4kpUFe5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_content['year'] = pd.to_datetime(all_content['date_added']).dt.year\n",
        "\n",
        "\n",
        "#Display in a graph the evolution curve of the number of contents added to the Netflix catalog. We will differentiate the type of content.\n",
        "\n",
        "\n",
        "\n",
        "sns.lineplot(x = 'year', y = 'show_id',data = all_content.groupby(['year']).count())\n",
        "\n",
        "#if you want by type\n",
        "sns.lineplot(x = 'year', y='show_id', hue ='type' , data =  all_content.groupby(['type','year']).count())"
      ],
      "metadata": {
        "id": "ScFDlXUMKNdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Réponses :\n",
        "\n",
        "# a)\n",
        "regions.head(10)\n",
        "\n",
        "# b)\n",
        "regions['Region'] = regions['Region'].apply(lambda x : x.upper())\n",
        "regions['Ville'] = regions['Ville'].apply(lambda x : x.upper())\n",
        "\n",
        "# c)\n",
        "## Affiche les différentes valeurs prises par la variable Region\n",
        "print(regions['Region'].unique())\n",
        "\n",
        "## Remplace en suivant le tableau de conversion\n",
        "regions.replace({'LANGUEDOC-ROUSSILLON' : 'OCCITANIE',\n",
        "                 ' AQUITAINE' : 'NOUVELLE-AQUITAINE', 'POITOU-CHARENTES': 'NOUVELLE-AQUITAINE',\n",
        "                 'RHONE-ALPES' : 'AUVERGNE-RHONE-ALPES'},\n",
        "               inplace = True)\n",
        "\n",
        "# d)\n",
        "## Affiche les différentes valeurs prises par la variable Ville\n",
        "print(regions['Ville'].unique())\n",
        "\n",
        "## On s'aperçoit que \"RENES\" et \"PERPINAN\" sont mals orthographiés, et que \"PARIS LA DÉFENSE\" fait doublon avec \"PARIS\"\n",
        "regions.replace({'RENES' : 'RENNES',\n",
        "                 ' PERPINAN' : 'PERPIGNAN',\n",
        "                 'PARIS LA DEFENSE' : 'PARIS'},\n",
        "               inplace = True)\n",
        "\n",
        "# e)\n",
        "Ventes_2017.head()\n",
        "Ventes_2018.head()\n",
        "Ventes_2019.head()\n",
        "# En 2017, les Id Clients ne sont pas représentés au même format que pour les années suivantes.\n",
        "# Il s'agit de chaînes de caractères contenant le préfixe 'Id' avant le numéro client.\n",
        "\n",
        "# f)\n",
        "Ventes_2017['Id Client'] = Ventes_2017['Id Client'].apply(lambda x : x[2:]).astype(int)\n",
        "\n",
        "# g)\n",
        "print(\"produits vendus en 2017 ;\", sorted(Ventes_2017['Id Produit'].unique()))\n",
        "print(\"produits vendus en 2018 :\", sorted(Ventes_2018['Id Produit'].unique()))\n",
        "\n",
        "# Les produits d'Id 2, 7, 8 et 11 ont été ajouté au catalogue en 2018\n",
        "\n",
        "# Autre méthode à l'aide des ensembles (utiles si vous avez beaucoup de produits différents) :\n",
        "# A = set(Ventes_2017['Id Produit'])\n",
        "# B = set(Ventes_2018['Id Produit'])\n",
        "# B - A\n",
        "\n",
        "# h)\n",
        "Ventes_2018['Date de Commande'] = pd.to_datetime(Ventes_2018['Date de Commande'])\n",
        "Ventes_2018['Date de Livraison'] = pd.to_datetime(Ventes_2018['Date de Livraison'])\n",
        "\n",
        "Ventes_2019['Date de Commande'] = pd.to_datetime(Ventes_2019['Date de Commande'])\n",
        "Ventes_2019['Date de Livraison'] = pd.to_datetime(Ventes_2019['Date de Livraison'])\n",
        "\n",
        "# i)\n",
        "Ventes_2017.duplicated().sum()\n",
        "Ventes_2018.duplicated().sum()\n",
        "Ventes_2019.duplicated().sum()\n",
        "Ventes_2018.drop_duplicates(inplace = True)\n",
        "\n",
        "# j)\n",
        "Ventes_globales = pd.concat([Ventes_2017,Ventes_2018,Ventes_2019])\n",
        "\n",
        "# k)\n",
        "Ventes_globales['Jour de Livraison'] = Ventes_globales['Date de Livraison'].dt.weekday\n",
        "Ventes_globales['Mois de Commande'] = Ventes_globales['Date de Commande'].dt.month\n",
        "\n",
        "# l)\n",
        "Ventes_globales[\"Nouveaux Produits\"] = Ventes_globales['Date de Commande'].dt.year==(2018|2019)\n",
        "\n",
        "# m)\n",
        "produits['Categorie'] = produits['Categorie 1'] + 2*produits['Categorie 2'] + 3*produits['Categorie 3']\n",
        "produits.head()\n",
        "\n",
        "# n)\n",
        "produits['Categorie'] = produits[\"Categorie\"].astype('str')\n",
        "produits.drop(['Categorie 1', 'Categorie 2', 'Categorie 3'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "rL87SIJp310C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}